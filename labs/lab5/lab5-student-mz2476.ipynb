{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Time Series Analysis\n",
    "\n",
    "## Week 5: Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following new Python package. Install it either with `conda install -c conda-forge` if you use the Anaconda environment or `pip install`.\n",
    "\n",
    "    hmmlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Places where you are supposed to fill in code are marked\n",
    "\n",
    "    #\n",
    "    # TODO: some instructions\n",
    "    # \n",
    "    \n",
    "The rest of the code we will run and discuss if time permits, otherwise try it out at home and try to answer the questions mentioned in the text boxes for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please turn in the code before 10/10/2018 5:20pm. \n",
    "\n",
    "### Your work will be evaluated based on the code and plots. You don't need to write down your answers to other questions in the text blocks, just think them over.\n",
    "\n",
    "### Title your submission file `lab5-student-[YOUR NET ID].ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import pylab\n",
    "from collections import Counter\n",
    "# from hmmlearn import hmm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Data Loading\n",
    "\n",
    "Load the Wall Street Journal POS dataset. Transform them into indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../../data/en-wsj-train.pos\"\n",
    "test_dir = \"../../data/en-wsj-dev.pos\"\n",
    "\n",
    "def load_pos_data(data_dir, word_indexer=None, label_indexer=None, top_k=20000):\n",
    "    \"\"\"\n",
    "    Function that loads the data\n",
    "    \"\"\"\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        # load data\n",
    "        content = f.readlines()\n",
    "        intermediate_X = []\n",
    "        intermediate_z = []\n",
    "        current_X = []\n",
    "        current_z = []\n",
    "        vocab_counter = Counter()\n",
    "        label_set = set()\n",
    "        for line in content:\n",
    "            tokens = line.replace(\"\\n\", \"\").replace(\"$\", \"\").split(\"\\t\")\n",
    "            if len(tokens) <= 1:\n",
    "                intermediate_X.append(current_X)\n",
    "                intermediate_z.append(current_z)\n",
    "                current_X = []\n",
    "                current_z = []\n",
    "            elif len(tokens[1]) > 0:\n",
    "                vocab_counter[tokens[0]]+=1\n",
    "                label_set.add(tokens[1])\n",
    "                current_X.append(tokens[0].lower())\n",
    "                current_z.append(tokens[1])\n",
    "        # index data\n",
    "        top_k_words = vocab_counter.most_common(top_k)\n",
    "        # 0 is reserved for unknown words\n",
    "        word_indexer = word_indexer if word_indexer is not None else dict([(top_k_words[i][0], i+1) for i in range(len(top_k_words))])\n",
    "        word_indexer[\"UNK\"] = 0 \n",
    "        label_indexer = label_indexer if label_indexer is not None else dict([(label, i) for i, label in enumerate(label_set)])\n",
    "        output_X = []\n",
    "        output_z = []\n",
    "        current_X = []\n",
    "        current_z = []\n",
    "        for i in range(len(intermediate_X)):\n",
    "            for j in range(len(intermediate_X[i])):\n",
    "                if intermediate_X[i][j] in word_indexer:\n",
    "                    current_X.append(word_indexer[intermediate_X[i][j]])\n",
    "                else:\n",
    "                    current_X.append(0)\n",
    "                current_z.append(label_indexer[intermediate_z[i][j]])\n",
    "            # populate holders\n",
    "            output_X.append(current_X)\n",
    "            output_z.append(current_z)\n",
    "            # reset current holder\n",
    "            current_X = []\n",
    "            current_z = []\n",
    "        return output_X, output_z, label_indexer, word_indexer, {v: k for k, v in label_indexer.items()}, {v: k for k, v in word_indexer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_z, label_indexer, word_indexer, label_lookup, vocab_lookup = load_pos_data(train_dir)\n",
    "test_X, test_z, _, _, _, _ = load_pos_data(test_dir, word_indexer=word_indexer, label_indexer=label_indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: HMM Implementation\n",
    "\n",
    "In this part, you will implement the Hidden Markov Model with following methods:\n",
    "\n",
    "\n",
    "- sample: given a initial state and the number of steps, returns a sequence of sampled states and observations.\n",
    "\n",
    "\n",
    "- fit: update the transition matrix, emission matrix, and the initial state probability. Note that in our case, the hidden states are given. We don't need to use EM for the learning. Just count the number of times that given transitions / emissions / initial states appear, and normalize those counts to fill in the parameters.\n",
    "\n",
    "\n",
    "- decode_single_chain: use the Viterbi Algorithm to find the most probable sequence of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_sequence(idx_list, lookup):\n",
    "    \"\"\"\n",
    "    Function that reconstructs a sequence of index\n",
    "    \"\"\"\n",
    "    return [lookup[x] for x in idx_list]\n",
    "\n",
    "def percentage_agree(x, y):\n",
    "    \"\"\"\n",
    "    Function that shows the % of agreement among two list\n",
    "    \"\"\"\n",
    "    assert len(x)==len(y)\n",
    "    return float(np.sum(np.array(x)==np.array(y)))/len(x)\n",
    "\n",
    "class MyHMM:\n",
    "    def __init__(self, num_unique_states, num_observations):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        @param num_unique_states: # of unique states (POS Tags)\n",
    "        @param num_observations: # of unique observations (words)\n",
    "        \"\"\"\n",
    "        self.EPS = 1e-100\n",
    "        self.num_unique_states = num_unique_states\n",
    "        self.num_observations = num_observations\n",
    "        self.transition_matrix = np.zeros((num_unique_states, num_unique_states))\n",
    "        self.emission_matrix = np.zeros((num_unique_states, num_observations))\n",
    "        self.initial_states_vector = np.zeros(num_unique_states)\n",
    "    \n",
    "    def fit(self, X, z):\n",
    "#         fit: update the transition matrix, emission matrix, \n",
    "# and the initial state probability. Note that in our case, \n",
    "#the hidden states are given. We don't need to use EM for the learning.\n",
    "#Just count the number of times that given transitions / emissions / \n",
    "#initial states appear, and normalize those counts to fill in \n",
    "#the parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        Method that fits the model.\n",
    "        @param X: array-like with dimension [# of examples, # of length]\n",
    "        @param z: array-like with dimension [# of examples, # of length]\n",
    "        \"\"\"\n",
    "        #\n",
    "        # TODO: Add your implementation here\n",
    "        #\n",
    "        assert(len(X) == len(z))\n",
    "        \n",
    "        for sentence_id in range(len(X)):\n",
    "            sentence = X[sentence_id]\n",
    "            tags     = z[sentence_id]\n",
    "            \n",
    "            # learn initial\n",
    "            self.initial_states_vector[tags[0]] += 1\n",
    "            \n",
    "            # learn tags and tags to word\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.transition_matrix[tags[i]][tags[i+1]] += 1\n",
    "                self.emission_matrix[tags[i]][sentence[i]] += 1\n",
    "                \n",
    "            # do not forget about the last one\n",
    "            self.emission_matrix[tags[-1]][sentence[-1]] += 1\n",
    "                \n",
    "        # normalizing\n",
    "        # obviously, we need some smoothing (like Laplace smoothing or others)\n",
    "        # to make it work better\n",
    "        # here we use simple counter approach\n",
    "        self.initial_states_vector /= self.initial_states_vector.sum()\n",
    "        \n",
    "        self.transition_matrix = (self.transition_matrix.T /\n",
    "                                  self.transition_matrix.sum(axis=1).T).T\n",
    "        self.emission_matrix = (self.emission_matrix.T / \n",
    "                                self.emission_matrix.sum(axis=1).T).T\n",
    "        \n",
    "        \n",
    "        # for computational stability and shorter names\n",
    "        self.log_z_0 = np.log(self.initial_states_vector + self.EPS)\n",
    "        self.log_z_z = np.log(self.transition_matrix + self.EPS)\n",
    "        self.log_z_x = np.log(self.emission_matrix + self.EPS)\n",
    "\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def decode_single_chain(self, sentence):\n",
    "        \"\"\"\n",
    "        Auxiliary method that uses Viterbi on single chain\n",
    "        @param X: array-like with dimension [ # of length]\n",
    "        @return z: array-like with dimension [# of length]\n",
    "        \"\"\"     \n",
    "        #\n",
    "        # TODO: Add your implementation here\n",
    "        #\n",
    "        \n",
    "        # we will use these matrices\n",
    "        prob_matrix = [[-1000000] * len(sentence) \n",
    "                       for _ in range(self.num_unique_states)\n",
    "                      ]\n",
    "        # to recover answer\n",
    "        back_matrix = [[-1] * len(sentence)  \n",
    "                       for _ in range(self.num_unique_states)\n",
    "                      ]\n",
    "\n",
    "        # initialization\n",
    "        for i in range(self.num_unique_states):\n",
    "            prob_matrix[i][0] = (self.log_z_0[i]\n",
    "                                 + self.log_z_x[i][sentence[0]])\n",
    "            \n",
    "        for column in range(1, len(sentence)):\n",
    "            current_word = sentence[column]\n",
    "            for current_tag in range(self.num_unique_states):\n",
    "                for prev_tag in range(self.num_unique_states):\n",
    "                    new_prob = (prob_matrix[prev_tag][column - 1]\n",
    "                                + self.log_z_z[prev_tag][current_tag]\n",
    "                                + self.log_z_x[current_tag][current_word])\n",
    "                    if new_prob > prob_matrix[current_tag][column] and new_prob:\n",
    "                        prob_matrix[current_tag][column] = new_prob\n",
    "                        back_matrix[current_tag][column] = prev_tag\n",
    "\n",
    "\n",
    "        result = []\n",
    "        result.append(np.argmax([row[-1] \n",
    "                                for row in prob_matrix]))\n",
    "\n",
    "        for i in range(len(sentence) - 1, 0, -1):\n",
    "            result.append(back_matrix[result[-1]][i])\n",
    "\n",
    "    #     print(tags)\n",
    "    #     print(*prob_matrix, sep='\\n')\n",
    "    #     print(*back_matrix, sep='\\n')\n",
    "        \n",
    "        return result[::-1]\n",
    "        \n",
    "    def decode(self, X):\n",
    "        \"\"\"\n",
    "        Method that performs the Viterbi the model.\n",
    "        @param X: array-like with dimension [# of examples, # of length]\n",
    "        @return z: array-like with dimension [# of examples, # of length]\n",
    "        \"\"\"\n",
    "        return [self.decode_single_chain(sample) for sample in X]\n",
    "    \n",
    "    def sample(self, n_step, initial_state):\n",
    "        \"\"\"\n",
    "        Method that given initial state and produces n_step states and observations\n",
    "        @param n_step: integer\n",
    "        @param initial_state: an integer indicating the state\n",
    "        \"\"\"\n",
    "        #\n",
    "        # TODO: Add your implementation here\n",
    "        #\n",
    "        states = []  # aka z\n",
    "        observations = []  # aka x\n",
    "        \n",
    "        cur_state = initial_state\n",
    "        cur_observation = np.random.choice(self.num_observations, \n",
    "                                           p=self.emission_matrix[cur_state])\n",
    "        \n",
    "        states.append(cur_state)\n",
    "        observations.append(cur_observation)\n",
    "        \n",
    "        for i in range(n_step-1):\n",
    "            cur_state = np.random.choice(self.num_unique_states, \n",
    "                                         p=self.transition_matrix[cur_state])\n",
    "            cur_observation = np.random.choice(self.num_observations, \n",
    "                                               p=self.emission_matrix[cur_state])\n",
    "        \n",
    "            states.append(cur_state)\n",
    "            observations.append(cur_observation)\n",
    "            \n",
    "        return states, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_HMM = MyHMM(num_unique_states=len(label_indexer), \n",
    "                   num_observations=len(word_indexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_HMM.fit(train_X, train_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-230.2585093 ,   -8.51255746,   -2.07061461,   -3.18711143,\n",
       "         -7.81941028,   -7.22470317,   -4.40579038,   -5.96702619,\n",
       "         -8.10709235, -230.2585093 ,   -5.62218571,   -2.58463194,\n",
       "         -6.41761173,   -7.1580118 ,   -7.95294168,   -7.25979449,\n",
       "         -7.06563848,   -5.84706688, -230.2585093 ,   -6.58466582,\n",
       "         -8.64608886,   -1.60492719,   -7.8839488 , -230.2585093 ,\n",
       "         -1.52818843,   -2.84095389,   -2.86666897,   -2.67409842,\n",
       "         -3.20367115,   -5.13241349,   -3.17802871,   -6.53155599,\n",
       "         -6.19754985,   -4.39963652,   -5.77171744,   -5.46803503,\n",
       "         -6.14934775,   -7.37312318,  -10.591999  ,   -5.91917017,\n",
       "       -230.2585093 ,   -5.82982507,   -5.07454611, -230.2585093 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_HMM.log_z_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_HMM.sample(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PDT', 'DT', 'NN', 'NN', 'IN', 'CD', 'NNS', 'TO', 'VB', 'DT']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_sequence(sample[0], label_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'a',\n",
       " 'revenue',\n",
       " 'proposal',\n",
       " 'between',\n",
       " '640',\n",
       " 'officials',\n",
       " 'to',\n",
       " 'invest',\n",
       " 'some']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_sequence(sample[1], vocab_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 25, 43, 43, 24, 3, 14, 13, 16, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_HMM.decode_single_chain(sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn an HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = len(label_indexer)\n",
    "num_obs = len(word_indexer)\n",
    "my_hmm = MyHMM(num_states, num_obs)\n",
    "my_hmm.fit(train_X, train_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['this', 'financing', 'system', 'was', 'created', 'in', 'the', 'new', 'law', 'in', 'order', 'to', 'keep', 'the', 'bailout', 'spending', 'from', 'swelling', 'the', 'budget', 'deficit', '.'] \n",
      " pred: ['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', '.'] \n",
      " label: ['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "res = my_hmm.decode_single_chain(test_X[i])\n",
    "print(\"data: {0} \\n pred: {1} \\n label: {2}\".format(reconstruct_sequence(test_X[i], vocab_lookup),\n",
    "                                                    reconstruct_sequence(res, label_lookup),\n",
    "                                                  reconstruct_sequence(test_z[i], label_lookup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takes 46.78345489501953 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_train = my_hmm.decode(train_X[:1000])\n",
    "print(\"takes {0} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takes 78.61477279663086 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_test = my_hmm.decode(test_X)\n",
    "print(\"takes {0} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9320476845713684"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([percentage_agree(pred_train[i], train_z[i]) for i in range(len(pred_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9204388549707038"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([percentage_agree(pred_test[i], test_z[i]) for i in range(len(pred_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'NN', ',', 'DT', 'NN', 'IN', 'CD', 'NNS', 'CC', 'DT']\n",
      "['UNK', 'stock', ',', 'the', 'UNK', 'in', 'billion', 'companies', 'and', 'the']\n"
     ]
    }
   ],
   "source": [
    "pos_tag, words = my_hmm.sample(10, label_indexer[\"NNP\"])\n",
    "print(reconstruct_sequence(pos_tag, label_lookup))\n",
    "print(reconstruct_sequence(words, vocab_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
