{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Time Series Analysis\n",
    "\n",
    "## Week 5: Recurrent Neural Networks\n",
    "\n",
    "Places where you are supposed to fill in code are marked\n",
    "\n",
    "    #\n",
    "    # TODO: some instructions\n",
    "    # \n",
    "    \n",
    "The rest of the code we will run and discuss if time permits, otherwise try it out at home and try to answer the questions mentioned in the text boxes for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: When you try different datasets, change the filename in this line:\n",
    "#\n",
    "# I've gathered some text files for you to try, but feel free to use the code on your own that you find online.\n",
    "# The prepared examples are (all in the data directory on Github):\n",
    "# - catcher.txt\n",
    "# - boh.txt\n",
    "# - holmes.txt\n",
    "# - war.txt\n",
    "# - quotes.txt\n",
    "data = open('../../data/catcher.txt', 'r').read()\n",
    "data_size = len(data)\n",
    "data = data + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: Create a one-hot encoding for characters that occur in your text file by following the instructions below.\n",
    "# \n",
    "\n",
    "# This should contain a list of unique characters:\n",
    "chars = None\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print 'Input data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "\n",
    "# These should be dictionaries of character-to-index in `chars` and index-to-character:\n",
    "char_to_ix = None\n",
    "ix_to_char = None\n",
    "\n",
    "# Some error checking for you\n",
    "assert set(char_to_ix.keys()) == set(ix_to_char.values())\n",
    "assert set(ix_to_char.keys()) == set(char_to_ix.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 100  # size of hidden layer of neurons\n",
    "seq_length = 50    # number of steps to unroll the RNN for\n",
    "learning_rate = 5e-2\n",
    "maxiters = 20000\n",
    "\n",
    "# Model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n, temp=1.0):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in xrange(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y / temp) / np.sum(np.exp(y / temp))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "unsmoothed_losses = []\n",
    "smoothed_losses = []\n",
    "gradients = []\n",
    "\n",
    "while n < maxiters:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    if n % 200 == 0:\n",
    "        unsmoothed_losses.append(loss)\n",
    "        smoothed_losses.append(smooth_loss)\n",
    "        gradients.append((dWxh, dWhh, dWhy, dbh, dby))\n",
    "    \n",
    "    if n % 500 == 0: \n",
    "        print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# TODO: The variables `unsmoothed_losses` and `smoothed_losses` contain periodically \n",
    "# sampled training losses, and in the latter we take a moving average. Plot these two\n",
    "# in this cell. What do you notice?\n",
    "#\n",
    "\n",
    "old_smoothed_losses = np.array(smoothed_losses)\n",
    "old_unsmoothed_losses = np.array(unsmoothed_losses)\n",
    "\n",
    "# Plot using the `old_` variables, since you're going to re-run the code below with\n",
    "# different parameters.\n",
    "plt.plot(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# TODO: Change the learning rate, run the training code again, and plot the new results\n",
    "# (leave the old results in the cell above). What do you notice? How could we tune this\n",
    "# hyperparameter? If you have time, try modifying the code to vary the learning rate\n",
    "# over time. See what happens.\n",
    "# \n",
    "\n",
    "# Now plot using the variables without `old_`, after re-running.\n",
    "plt.plot(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: The variable `gradients` is a list of tuples of gradients for each of the variables\n",
    "# in the network (some are matrices). Compute the norms of the gradients of the variables\n",
    "# at each time step and make plots (use np.linalg.norm(X, ord='fro') for matrices). What\n",
    "# do you notice? Do you observe convergence? You might want to try smoothing the gradient norms.\n",
    "#\n",
    "\n",
    "# Example of contents of gradients:\n",
    "for value, name in zip(gradients[0], ['dWxh', 'dWhh', 'dWhy', 'dbh', 'dby']):\n",
    "    print name, ':'\n",
    "    print value\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: Here is an example of sampling from the trained network. The parameter `temp` can\n",
    "# be tuned to vary the probability distribution from which we sample. The default is 1.0.\n",
    "# Pass in different values and print a few samples with each. What's the difference?\n",
    "#\n",
    "\n",
    "# Example of sampling:\n",
    "sample_ix = sample(hprev, inputs[0], 200, temp=1.0)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "These lab materials are based on Andrej Karpathy's blog post and implementation:\n",
    "\n",
    "https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
